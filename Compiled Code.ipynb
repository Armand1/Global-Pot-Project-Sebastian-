{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below extracts the image associated with the mask in the training set and saves it to an assigned directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_and_save_image(image_url, image_path):\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(image_path, \"wb\") as img_file:\n",
    "            img_file.write(response.content)\n",
    "\n",
    "def process_entry(entry):\n",
    "    image_id = entry['data_row']['external_id']\n",
    "    image_url = entry['data_row']['row_data']\n",
    "\n",
    "\n",
    "    file_extension = os.path.splitext(image_id)[1]\n",
    "\n",
    "    image_path = os.path.join(image_directory_path, f\"{image_id}\")\n",
    "\n",
    "\n",
    "    download_and_save_image(image_url, image_path)\n",
    "\n",
    "\n",
    "# Set the paths\n",
    "json_file_path = r\"C:\\Users\\sebca\\Downloads\\actual_complete_training_dataset.json\"\n",
    "image_directory_path = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Image Directory\"\n",
    "\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(image_directory_path, exist_ok=True)\n",
    "\n",
    "# Read JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Use a ThreadPoolExecutor to process entries concurrently\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_entry, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes the json files containing the mask data from Labelbox and saves them as pngs to a folder of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "import concurrent.futures\n",
    "#Downloads the masks using the urls given in the json file, but requires an API key in order to do so.\n",
    "def download_mask(url, save_path, api_keys, retries=3, delay=5):\n",
    "    for api_key in api_keys:\n",
    "        headers = {'Authorization': f'Bearer {api_key}'}\n",
    "        \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "#Occasionally, it may take too long for the urls to load. This ensures that the masks are saved when this happens.\n",
    "                if response.status_code == 200:\n",
    "                    with open(save_path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    return\n",
    "                elif response.status_code != 403:  # Not an authentication error, raises the exception\n",
    "                    response.raise_for_status()\n",
    "            \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code >= 500:  # Server-side error\n",
    "                    if attempt < retries - 1:  # Not the last attempt\n",
    "                        print(f\"Server-side error ({e}), retrying in {delay} seconds...\")\n",
    "                        time.sleep(delay)\n",
    "                    else:  # Last attempt, re-raises the exception\n",
    "                        raise e\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    raise Exception(\"Failed to download mask with all provided API keys\")\n",
    "#Finds the mask URL and extracts it for saving to the assigned directory.\n",
    "def process_labelbox_data(labelbox_data):\n",
    "    external_id = labelbox_data['data_row']['external_id']\n",
    "    projects = labelbox_data['projects']\n",
    "\n",
    "    for project_id, project_data in projects.items():\n",
    "        labels = project_data['labels']\n",
    "\n",
    "        for label in labels:\n",
    "            annotations = label['annotations']\n",
    "            objects = annotations['objects']\n",
    "\n",
    "            for obj in objects:\n",
    "                if obj['annotation_kind'] == 'ImageSegmentationMask':\n",
    "                    mask_url = obj['mask']['url']\n",
    "                    mask_filename = f\"{external_id}_mask.png\"\n",
    "                    save_path = mask_output_folder / mask_filename\n",
    "                    print(f\"Downloading mask for {external_id}...\")\n",
    "                    download_mask(mask_url, save_path, api_keys)\n",
    "                    print(f\"Downloaded mask for {external_id} to {save_path}\")\n",
    "\n",
    "# Loads your Labelbox JSON file\n",
    "labelbox_export_file = Path(r\"C:\\Users\\sebca\\Downloads\\actual_complete_training_dataset.json\")\n",
    "with open(labelbox_export_file, 'r') as file:\n",
    "    labelbox_data_list = json.load(file)\n",
    "\n",
    "mask_output_folder = Path(r'C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Mask Directory')\n",
    "\n",
    "# Replace with your Labelbox API key - if using multiple accounts, you need to use multiple keys\n",
    "api_keys = [ Key1, \n",
    "            Key2, \n",
    "            Key3\n",
    "]\n",
    "\n",
    "# Creates the output folder if it doesn't exist\n",
    "mask_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use ThreadPoolExecutor to process labelbox_data_list concurrently - makes the process a lot faster.\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_labelbox_data, labelbox_data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is responsible for creating and training the initial machine learning model that can then be further trained later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Loads and preprocesses the training dataset\n",
    "def load_dataset(image_dir, mask_dir):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for file in os.listdir(image_dir):\n",
    "        if not file.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing file: {file}\") \n",
    "        #Reads the image in grayscale and resizs them\n",
    "        img_path = os.path.join(image_dir, file)\n",
    "        print(f\"Image path: {img_path}\")  \n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        img = img / 255.0\n",
    "        \n",
    "        mask_filename = file.replace(\".jpg\", \".png\")  # Replaces .jpg with .png for mask files\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        print(f\"Mask path: {mask_path}\")  \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask is None:\n",
    "            print(f\"Unable to read mask file: {mask_path}\")\n",
    "            continue\n",
    "       # Resizes and normalises the masks\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    print(f\"Loaded {len(images)} images and {len(masks)} masks.\")\n",
    "    #Returns the lists as numpy arrays\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "#Defines the model architecture, which was selected as it was easy to run on standard hardware and is fairly standard.\n",
    "def unet_model(input_size=(256, 256, 1)):\n",
    "    inputs = tf.keras.Input(input_size)\n",
    "    #Defines the encoding path\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    #Defines the decoding path\n",
    "    u2 = UpSampling2D(size=(2, 2))(c2)\n",
    "    u2 = Concatenate(axis=3)([u2, c1])\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(c3)\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c3)\n",
    "    #Returns the model\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "#Specifies the directories containing the images and masks for training.\n",
    "image_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Image Directory\"\n",
    "mask_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Mask Directory\"\n",
    "\n",
    "images, masks = load_dataset(image_dir, mask_dir)\n",
    "\n",
    "# Splits the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creates and compiles the U-Net model\n",
    "model = unet_model()\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Creates a TensorBoard callback\n",
    "log_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\logs\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Trains the model with the TensorBoard callback, which is incredibly useful for tracking the progress of the model.\n",
    "model.fit(X_train, y_train, batch_size=8, epochs=40, validation_data=(X_test, y_test), callbacks=[tensorboard_callback], verbose=1)\n",
    "\n",
    "# Saves the trained model at the specified location for loading at a later date.\n",
    "model.save(r'C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\pot_segmentation_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below preprocesses the training images, trains the model that already has been created, and then runs the algorithm on the input images so to generate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#Defines the training dataset and preprocesses it so it can be fed into the algorithm\n",
    "def load_dataset(image_dir, mask_dir):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for file in os.listdir(image_dir):\n",
    "        if not file.endswith(\".png\"): #Change to input image type when needed\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        img = img / 255.0\n",
    "\n",
    "        mask_filename = file.replace(\".jpg\", \".png\")  # Replace .jpg with .png for mask files\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "#Defines the predicted images should look like and what they should be called.\n",
    "def save_predicted_image(input_image, prediction, input_filename, output_dir):\n",
    "    img = input_image.reshape((256, 256))\n",
    "    pred = prediction.reshape((256, 256))\n",
    "    pred = cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    _, pred = cv2.threshold(pred, 127, 255, cv2.THRESH_BINARY)\n",
    "    pred = 255 - pred  # Invert the binary mask\n",
    "    output_filename = os.path.splitext(input_filename)[0] + '.png'\n",
    "    \n",
    "    # Resize the prediction to 512x512 before saving\n",
    "    pred_resized = cv2.resize(pred, (512, 512))\n",
    "    cv2.imwrite(os.path.join(output_dir, output_filename), pred_resized)\n",
    "#Creates the batches that all the input images are gathered under so that the algorithm can process them without overloading the memory.\n",
    "def process_batch(images_batch):\n",
    "    images_batch = np.array(images_batch) / 255.0\n",
    "    images_batch = images_batch.reshape(images_batch.shape[0], 256, 256, 1)\n",
    "    predictions = get_output([images_batch])[0]\n",
    "    return predictions\n",
    "\n",
    "# Load the training dataset\n",
    "image_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Training data for cleaning\"\n",
    "mask_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Mask Directory\"\n",
    "\n",
    "images, masks = load_dataset(image_dir, mask_dir)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loads the model and trains it\n",
    "model_path = r'C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\pot_segmentation_model_cleaning.h5'\n",
    "model = load_model(model_path)\n",
    "#Outputs the loss and accuracy values to a folder, as well as creates a visual interface with which to watch the models progress as it trains \n",
    "log_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\logs\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#Defines what the training cycles look like - batch size indicates the number of images being processed at any one time, and the number of epochs is how many times the model goes over the complete dataset.\n",
    "model.fit(X_train, y_train, batch_size=15, epochs=15, validation_data=(X_test, y_test), callbacks=[tensorboard_callback], verbose=1)\n",
    "\n",
    "# Saves the trained model\n",
    "model.save(model_path)\n",
    "\n",
    "# Loads the input images that the trained model is to process\n",
    "input_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Processed Images\"\n",
    "output_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Cleaned Images New\"\n",
    "\n",
    "#Ensures that the images are standardised in terms of size before loading and that they are grayscale. \n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []  # List to store the filenames\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (256, 256))  # Resizes the image to the desired shape\n",
    "            images.append(img)\n",
    "            filenames.append(filename)  # Adds the filename to the list in the order that they were loaded.\n",
    "    return images, filenames\n",
    "\n",
    "test_images, input_filenames = load_images_from_folder(input_dir)\n",
    "print(\"Loaded test images: \", len(test_images))\n",
    "\n",
    "# Loads the model again for prediction\n",
    "model = load_model(model_path)\n",
    "model.compile(loss='binary_crossentropy')\n",
    "\n",
    "print(\"Model loaded and compiled.\")\n",
    "\n",
    "get_output = K.function([model.layers[0].input], [model.layers[-1].output])\n",
    "print(\"Custom predict function created.\")\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 2\n",
    "n_batches = len(test_images) // batch_size + (1 if len(test_images) % batch_size != 0 else 0)\n",
    "\n",
    "all_predictions = []\n",
    "for batch_idx in range(n_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "    batch_images = test_images[start_idx:end_idx]\n",
    "    batch_filenames = input_filenames[start_idx:end_idx]\n",
    "    print(f\"Processing batch {batch_idx + 1}/{n_batches}\")\n",
    "    batch_predictions = process_batch(batch_images)\n",
    "    for i in range(len(batch_predictions)):\n",
    "        save_predicted_image(batch_images[i], batch_predictions[i], batch_filenames[i], output_dir)\n",
    "\n",
    "print(\"Predicted images saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes processes the original training images into predictions, then outputs them into a new directory to serve as training data for the cleaning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#Defines the training dataset and preprocesses it so it can be fed into the algorithm\n",
    "def load_dataset(image_dir, mask_dir):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for file in os.listdir(image_dir):\n",
    "        if not file.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        img = img / 255.0\n",
    "\n",
    "        mask_filename = file.replace(\".jpg\", \".png\")  # Replace .jpg with .png for mask files\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "#Defines the predicted images should look like and what they should be called.\n",
    "def save_predicted_image(input_image, prediction, input_filename, output_dir):\n",
    "    img = input_image.reshape((256, 256))\n",
    "    pred = prediction.reshape((256, 256))\n",
    "    pred = cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    _, pred = cv2.threshold(pred, 127, 255, cv2.THRESH_BINARY)\n",
    "    pred = 255 - pred  # Invert the binary mask\n",
    "    output_filename = os.path.splitext(input_filename)[0] + '.png'\n",
    "    \n",
    "    # Resize the prediction to 512x512 before saving\n",
    "    pred_resized = cv2.resize(pred, (512, 512))\n",
    "    cv2.imwrite(os.path.join(output_dir, output_filename), pred_resized)\n",
    "#Creates the batches that all the input images are gathered under so that the algorithm can process them without overloading the memory.\n",
    "def process_batch(images_batch):\n",
    "    images_batch = np.array(images_batch) / 255.0\n",
    "    images_batch = images_batch.reshape(images_batch.shape[0], 256, 256, 1)\n",
    "    predictions = get_output([images_batch])[0]\n",
    "    return predictions\n",
    "\n",
    "# Load the training dataset\n",
    "image_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Image Directory\"\n",
    "mask_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Mask Directory\"\n",
    "\n",
    "images, masks = load_dataset(image_dir, mask_dir)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loads the model and trains it\n",
    "model_path = r'C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\pot_segmentation_model.h5'\n",
    "model = load_model(model_path)\n",
    "#Outputs the loss and accuracy values to a folder, as well as creates a visual interface with which to watch the models progress as it trains \n",
    "log_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\logs\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#Defines what the training cycles look like - batch size indicates the number of images being processed at any one time, and the number of epochs is how many times the model goes over the complete dataset.\n",
    "#model.fit(X_train, y_train, batch_size=15, epochs=1, validation_data=(X_test, y_test), callbacks=[tensorboard_callback], verbose=1)\n",
    "\n",
    "# Saves the trained model\n",
    "model.save(model_path)\n",
    "\n",
    "# Loads the input images that the trained model is to process\n",
    "input_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Image Directory\"\n",
    "output_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Training data for cleaning\"\n",
    "\n",
    "#Ensures that the images are standardised in terms of size before loading and that they are grayscale. \n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []  # List to store the filenames\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (256, 256))  # Resizes the image to the desired shape\n",
    "            images.append(img)\n",
    "            filenames.append(filename)  # Adds the filename to the list in the order that they were loaded.\n",
    "    return images, filenames\n",
    "\n",
    "test_images, input_filenames = load_images_from_folder(input_dir)\n",
    "print(\"Loaded test images: \", len(test_images))\n",
    "\n",
    "# Loads the model again for prediction\n",
    "model = load_model(model_path)\n",
    "model.compile(loss='binary_crossentropy')\n",
    "\n",
    "print(\"Model loaded and compiled.\")\n",
    "\n",
    "get_output = K.function([model.layers[0].input], [model.layers[-1].output])\n",
    "print(\"Custom predict function created.\")\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 2\n",
    "n_batches = len(test_images) // batch_size + (1 if len(test_images) % batch_size != 0 else 0)\n",
    "\n",
    "all_predictions = []\n",
    "for batch_idx in range(n_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "    batch_images = test_images[start_idx:end_idx]\n",
    "    batch_filenames = input_filenames[start_idx:end_idx]\n",
    "    print(f\"Processing batch {batch_idx + 1}/{n_batches}\")\n",
    "    batch_predictions = process_batch(batch_images)\n",
    "    for i in range(len(batch_predictions)):\n",
    "        save_predicted_image(batch_images[i], batch_predictions[i], batch_filenames[i], output_dir)\n",
    "\n",
    "print(\"Predicted images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a machine learning model designed to specifically clean up the code. It then trains it, preprocesses the files to be cleaned, then generates the clean results as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Defines the training dataset and preprocesses it so it can be fed into the algorithm\n",
    "def load_dataset(image_dir, mask_dir):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for file in os.listdir(image_dir):\n",
    "        if not file.endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        img = img / 255.0\n",
    "\n",
    "        mask_filename = file.replace(\".jpg\", \".png\")  # Replace .jpg with .png for mask files\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "#Defines the model architecture\n",
    "def unet_model(input_size=(256, 256, 1)):\n",
    "    inputs = tf.keras.Input(input_size)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    u2 = UpSampling2D(size=(2, 2))(c2)\n",
    "    u2 = Concatenate(axis=3)([u2, c1])\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(c3)\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c3)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Load the training dataset\n",
    "image_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Training data for cleaning\"\n",
    "mask_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 4\\Mask Directory\"\n",
    "\n",
    "images, masks = load_dataset(image_dir, mask_dir)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creates and compiles the U-Net model\n",
    "model = unet_model()\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\logs\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Trains the model with the TensorBoard callback\n",
    "model.fit(X_train, y_train, batch_size=8, epochs=15, validation_data=(X_test, y_test), callbacks=[tensorboard_callback], verbose=1)\n",
    "\n",
    "# Saves the trained model at the specified location for loading at a later date.\n",
    "model_path = r'C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\pot_segmentation_model_cleaning.h5'\n",
    "model.save(model_path)\n",
    "\n",
    "#Defines the predicted images should look like and what they should be called.\n",
    "def save_predicted_image(input_image, prediction, input_filename, output_dir):\n",
    "    img = input_image.reshape((256, 256))\n",
    "    pred = prediction.reshape((256, 256))\n",
    "    pred = cv2.normalize(pred, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    _, pred = cv2.threshold(pred, 127, 255, cv2.THRESH_BINARY)\n",
    "    pred = 255 - pred  # Invert the binary mask\n",
    "    output_filename = os.path.splitext(input_filename)[0] + '.png'\n",
    "\n",
    "    # Resize the prediction to 512x512 before saving\n",
    "    pred_resized = cv2.resize(pred, (512, 512))\n",
    "    cv2.imwrite(os.path.join(output_dir, output_filename), pred_resized)\n",
    "\n",
    "#Creates the batches that all the input images are gathered under so that the algorithm can process them without overloading the memory.\n",
    "def process_batch(images_batch):\n",
    "    images_batch = np.array(images_batch) / 255.0\n",
    "    images_batch = images_batch.reshape(images_batch.shape[0], 256, 256, 1)\n",
    "    predictions = get_output([images_batch])[0]\n",
    "    return predictions\n",
    "\n",
    "# Loads the input images that the trained model is to process\n",
    "input_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Processed Images\"\n",
    "output_dir = r\"C:\\Users\\sebca\\OneDrive\\Final Year Project\\Week 5\\Cleaned Images\"\n",
    "\n",
    "#Ensures that the images are standardised in terms of size before loading and that they are grayscale. \n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []  # List to store the filenames\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (256, 256))  # Resizes the image to the desired shape\n",
    "            images.append(img)\n",
    "            filenames.append(filename)  # Adds the filename to the list in the order that they were loaded.\n",
    "    return images, filenames\n",
    "\n",
    "test_images, input_filenames = load_images_from_folder(input_dir)\n",
    "print(\"Loaded test images: \", len(test_images))\n",
    "\n",
    "# Loads the model again for prediction\n",
    "model = load_model(model_path)\n",
    "model.compile(loss='binary_crossentropy')\n",
    "\n",
    "print(\"Model loaded and compiled.\")\n",
    "\n",
    "get_output = K.function([model.layers[0].input], [model.layers[-1].output])\n",
    "print(\"Custom predict function created.\")\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 2\n",
    "n_batches = len(test_images) // batch_size + (1 if len(test_images) % batch_size != 0 else 0)\n",
    "\n",
    "all_predictions = []\n",
    "for batch_idx in range(n_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "    batch_images = test_images[start_idx:end_idx]\n",
    "    batch_filenames = input_filenames[start_idx:end_idx]\n",
    "    print(f\"Processing batch {batch_idx + 1}/{n_batches}\")\n",
    "    batch_predictions = process_batch(batch_images)\n",
    "    for i in range(len(batch_predictions)):\n",
    "        save_predicted_image(batch_images[i], batch_predictions[i], batch_filenames[i], output_dir)\n",
    "\n",
    "print(\"Predicted images saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
